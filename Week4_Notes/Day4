# Week 4 Day 4

# Boosting

## Objectives
- List features & benefits of boosting
- Compare & contrast boosting vs. other ensemble methods
- Learn, tune, and score a model using sklearn boosting classes
- implement the AdaBoost (additive boosting) algorithm

### Benefits of Bagging/Random Forest
- prevents overfitting
- variance goes down and bias stays the same
- we are averaging many uncorrelated trees (classifiers)
- this works great with trees that are low bias and high variance
- **Boosting** is used to lower the bias
- boosting often provides best in class performance

- we address bias by choosing the right model
- we can address variance by bagging, or using some sort of ensemble method to average out the high variance, this will result in a lower bias and a lower variance
- **For Random Forests:** we want to always use trees that are deep enough to sufficiently capture the information in our data, usually we want something 3 levels deep or more
- very deep trees are extremely variant models

### Boosting
- uses 'weak learners', basically crappy models that are a little bit better than random guessing
- use a stump (crappy learner), there will be a lot of errors
- train the next stump on the errors from the previous one
- the stumps are NOT independent because the decisions depend on the decisions of the others
- because they are correlated the variance is going to go up but the bias will go down a lot
- trees are slow to train but fast to predict
#### There are two main Boosting Algorithms
1. AdaBoost
2. Gradient Boosting Regression Trees

- In practice:
  1. Choose suitable family of weak learners and loss function for problem and data
  2. Fit using cross-validation
  3. Use grid search to optimize
  4. Evaluate
